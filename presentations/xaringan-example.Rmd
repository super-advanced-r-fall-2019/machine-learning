---
title: "Intro to Machine Learning with R"
subtitle: "Very Intro."
author: "Dan Ovando"
institute: "University of Washington"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align = "center", dev = "svg")
library(tidyverse)
library(tidymodels)
library(here)
library(knitr)
```


# Intro to Machine Learning: Objectives

We only have three hours! You won't leave here an expert in ML

My goals for you: 

1. Understand the motivation for machine learning

2. Understand the basic workflow of predictive modeling

3. Gain familiarity with the core predictive modeling tools in R

4. Develop some healthy skepticism around ML

---

# What is Machine Learning?

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("https://imgs.xkcd.com/comics/machine_learning.png")
```


---

# What is Machine Learning?


.pull-left[

Artificial Intellegence has been aroudn for a long time!

>The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform.... Its province is to assist us in making available what we’re already acquainted with
>
> `r tufte::quote_footer('--- Lady Ada Lovelace, 1843')`

]
.pull-right[
`r include_graphics(here::here("presentations","imgs","ai-ml-dl.png"))`
]

.footnote[
Chollet & Allaire 2018 - Deep Learning with R
]

???
In reference to a mechanical computer called the Analytical Engine, referenced by Alan Turing
---

# What is Machine Learning?


.pull-left[

Machine learning advanced structural AI by asking whether computers could **learn** new tasks. 

- "Classical" programming says "if email contains 'get rich quick!' mark as spam

- Machine learning asks given known examples of spam, can computer find rules to identify it?

]
.pull-right[
`r include_graphics(here::here("presentations","imgs","ai-ml-dl.png"))`
]

.footnote[
Chollet & Allaire 2018 - Deep Learning with R
]

???
In reference to a mechanical computer called the Analytical Engine, referenced by Alan Turing
---


# ML for Predictive Modeling

The primary application for machine learning is **prediction**, not "understanding"

Modeling for Prediction: "Will it rain?"
  - Are people carrying umbrellas? Probably gonna rain. 
  - Test by out-of-sample prediction
  - Realm of machine learning
  
Modeling for Inference: "Why does it rain?"
  - Probably not because of umbrellas. 
  - Test by experimentation
  - Realm of statistics
  
Correlation does not equal causation, but some correlations can be great predictors  

**Big red flag: Works claiming to use machine learning for "why" questions**

???

IMO, ecology sometimes has a bad habit of worst of both worlds: low R<sup>2</sup> and no identification strategy

---

# The Predctive Modeling Workflow

1. Obtain labeled dataset

2. Split into training and testing sets
  - Lock the testing set
  
3. Use the training set to train and select final model
  - Split into analysis and assessment sets

4. Apply final model to testing set and see how you did


---

# Predictive Modeling

"Predictive" nature of most ML makes it much more engineering than theory focused.

The proof is in the pudding: If it predicts well, who cares why?

The focus goes from identification strategies and probability theory to

- How good is the training dataset?

- How valid is the training/testing 

---

# Predictive Modeling
I sometimes use ML as a test for "ceiling" of predictability given data

  - Helps learn how much prediction insight is costing you

```{r, out.height=400, echo = FALSE}
 include_graphics(here::here("presentations","imgs","skynet.png"))
```

---

# Predictive Modeling

Predictive modeling falls under "supervised learning" (as opposed to unsupervised, things like clustering algorithms)

Regression (continuous response)
  - Predict presence of marine mammals
  - Predict sales on a given day
  - Predict salmon returns
  - Judged by things like mean squared error

Classification (categorical response)
  - What kind of fish is this?
  - Does a person have a disease?
  - Judged by things like classification accuracy

???
This branch of machine learning consists of finding interesting transformations of the input data without the help of any targets, for the purposes of data visualization, data compression, or data denoising, or to better understand the correlations present in the data at hand

Think PCA

---

# A Bestiary of Machine Learning Models

.pull-left[
Nearly all ML methods can be used for regression or classification

- Random forests
  - [`ranger`](https://github.com/imbs-hl/ranger)

- Gradient boosting machines
  - [`xgboost`](https://xgboost.readthedocs.io/en/latest/)

- Neural Networks
  - [`keras/TensorFlow`](https://tensorflow.rstudio.com/)
] .pull-right[

There are lots of them....

```{r}
caret::getModelInfo() %>% names()
```


]

---

# Random Forests

.pull-left[
Random forests (Breiman 2001) are a realiable workhorse of the ML world
- Usually not the best
- Rarely the worst
- Pretty hard to mess up too badly

Random forests are ensembles of regression trees

  - Algorithm learns split variables and breakpoints
  
  - Each tree is low-bias, high-variance
  
  - A "forest" of trees is low-bias, low(er)-variance


] .pull-right[

`r include_graphics(here::here("presentations","imgs","nico.png"))`

.footnote[Gutiérrez, Hilborn, & Defeo 2011]

]

---


# Random Forests

```{r, out.height=400, echo = FALSE}
 include_graphics(here::here("presentations","imgs","rf.png"))
```

.footnote[Kuhn & Johnson 2016]

---

# The Predictive Modeling Framework

1. Split into testing and training data. Split training data into analysis and assessment splits
  - [`rsample`](https://tidymodels.github.io/rsample/)

2. Preprocess data in analysis splits
  - [`recipes`](https://tidymodels.github.io/recipes/)
  - For extra credit, blind the data
  
3. Fit models to analysis splits, test on assessment
  - [`purrr`](https://purrr.tidyverse.org/)
  - [`parsnip`](https://www.tidyverse.org/blog/2018/11/parsnip-0-0-1/)
  - [`yardstick`](https://tidymodels.github.io/yardstick/)
  
4. Fit selected model on preprocessed training data, test on testing data
  - [`parsnip`](https://www.tidyverse.org/blog/2018/11/parsnip-0-0-1/)
  - Cross fingers


---

# The Predictive Modeling Framework


```{r, echo = FALSE}
include_graphics(here::here("presentations","imgs","flow.png"))
```

.footnote[From Max Kuhn's Predictive Modeling Workshop]

---

# Why R for ML?

I thought I was supposed to use Python for machine learning?

Very few (no?) ML models are actually **written** in R. Most in versions of C++/Python/CUDA. 

But, nearly all have interfaces with other languages. That means you can do all the data-wrangling in R, pass to ML models, then process and plot results in R

  - R is really good at this
  
Python is more common for these things so a bit more documentation, but documentation is also denser. 
  - Might be worth exploring if you're dealing with big data


---

# Our example Data

We're going to use some data on abalone ages from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Abalone), accessed through the `AppliedPredictiveModeling` package. 

```{r}
library(AppliedPredictiveModeling)
data(abalone)
abalone <- abalone %>% 
  janitor::clean_names()
glimpse(abalone)
```


---


# Abalone Ages

Age in years is rings + 1.5

.pull-left[
```{r}

abalone <- abalone %>% 
  mutate(age = rings + 1.5) %>% 
  select(-rings)

abalone %>% 
  ggplot(aes(longest_shell, age, fill = type)) + 
  geom_point(shape  = 21, alpha = 0.75) -> abplot

```
] .pull-right[

```{r, echo = FALSE}
abplot
```

]

---


# Step 1a: Split into testing and training

We've got a solid amount of data here, so let's hold out 25% of our data for "testing"

Only real advantage of rsample::initial_split is memory (and some nice time features)

```{r}
abalone_split <- rsample::initial_split(abalone, prop = 0.75)
abalone_split
```

Could also do stratified sampling if desired

```{r}
abalone_strata_split <-
  rsample::initial_split(abalone,
                         prop = 0.9,
                         strata = "age",
                         breaks = 6)

```

---


# Step 1b: Analysis and Assessment splits


```{r, echo = FALSE}

include_graphics(here("presentations","imgs","overfit.png"))

```

---

# Step 1b: Analysis and Assessment splits

We now need to break our training data into analysis and assessment splits. 

We'll use these splits to tune and compare candidate models. 
  - How deep should the tree go?
  - How many layers should our Neural Net have?

There are lots of options for this most common being k(v)-fold cross validation 

```{r, echo = FALSE}

include_graphics(here("presentations","imgs","vfold.png"))


```

.footnote[Kuhn & Johnson 2016]

---


# Step 1b: Analysis and Assessment splits


```{r}

abalone_vfold <- abalone_split %>% 
  rsample::training() %>% 
  rsample::vfold_cv(v = 5, repeats = 2)

abalone_vfold 

```

---


# Step 2: Analysis and Assessment splits

Can also stratify if we're worried about rare ages


```{r}

abalone_vfold <- abalone_split %>% 
  rsample::training() %>% 
  rsample::vfold_cv(v = 5, repeats = 2, strata = age, breaks = 6)

abalone_vfold %>% 
  mutate(dat = map(splits, analysis))

```

---

# Step 2: Preprocessing (feature engineering)

Statistics usually refers to transformations/encodings etc. as "preprocessing"

Just because, computer science usually calls if "feature engineering"

Examples

  - Centering and scaling
  
  - PCA
  
  - Dummy variables
  
  - Interactions
  
  - Imputation
  
How can we do this on the abalone data?

---


# Step 2: Preprocessing

**Absolutely critical to avoid data leakages**

Common mistake: Center and scale the data, then split, train etc. 

  - The "testing" data has leaked into the mean used in the "training" data
  
We always want to do our preprocessing on the most "inner" step of our resampling, e.g. each v-fold split. 

enter `recipes`!


```{r, echo = FALSE}
include_graphics(here("presentations","imgs","recipes.png"))

```


---


# `recipes`

`recipes` is an incredibly handy package for breaking preprocssing steps into a series of pipable operations

  - Allows for transformations from training data to be applied to testing data
  
  - For now, let's pull out one "analysis" split
  
```{r}

analysis_data <- rsample::analysis(abalone_vfold$splits[[1]])

assessment_data <- rsample::assessment(abalone_vfold$splits[[1]])

age_recipe <- recipe(age ~ ., data = analysis_data)

age_recipe

```

---

# wtf

---

# `recipes`

Preprocessing operations are specified by `step_` functions

Suppose we want to center and scale all the numeric predictors, and add dummy variables for "type


```{r}

age_recipe <- recipe(age ~ ., data = analysis_data) %>% 
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(),-all_outcomes()) %>% 
  step_dummy(all_nominal())

age_recipe

```

---

# `recipes`

We now have a portable recipe that we can apply to new datasets. 

To follow the theme, we can now "prepare" (`prep`) the recipe using our analysis data. 

Prepping calculates and stores the steps of the recipe using the supplied data 


```{r}

prepped_abalone <- prep(age_recipe, data = analysis_data)

prepped_abalone
```


---

# `recipes`

Once we've prepped our recipe, we can now apply it to data using `bake`


```{r}

baked_abalone <- bake(prepped_abalone, new_data = analysis_data)

glimpse(baked_abalone)

```

---


# `recipes`

Importantly, `bake` allows you to apply the steps "prepped" from one dataset to another!

In this case, we can now bake our assessment data using, using the means and standard deviations from the analysis split contained in our prepped recipe.  

```{r}

baked_assessment_abalone <- bake(prepped_abalone, new_data = assessment_data)

glimpse(baked_assessment_abalone)

```

---


# Exercise

Explore alternative `step_` functions in recipes. What does step_knn do?

---


# Step 3 Tune and Select Models

OK we're getting pretty close. We now have a preprocessed analysis and assessment splits. 

What should we use? We saw that there are **tons** of ML models out there, and different versions of the same model. 

  - Not always clear reasons to pick one over the other
  
We want to try lots of different models, but this can be a pain....

---

# Step 3 Tune and Select Models

```{r, echo = TRUE, eval = FALSE}
# From randomForest
rf_1 <-
  randomForest(x,y,mtry = 12,ntree = 2000,importance = TRUE)

# From ranger
rf_2 <- ranger(
  y ~ ., 
  data = dat, 
  mtry = 12, 
  num.trees = 2000, 
  importance = 'impurity'
)

# From sparklyr
rf_3 <- ml_random_forest(
  dat, 
  intercept = FALSE, 
  response = "y", 
  features = names(dat)[names(dat) != "y"], 
  col.sample.rate = 12,
  num.trees = 2000
)
```
---


# enter `parsnip`

The old solution to this problem was `caret`.
  - Centralized package for tuning and fitting models
  - Bit of a black box
  - Inconsistent naming conventions
  
`parsnip` to the rescue!

```{r, echo = FALSE}
include_graphics(here("presentations","imgs","parsnip.png"))
```

---


# `parsnip`

`parsnip` breaks the model fitting process into a series of pipable steps. 
  - Also standardized naming imputs!
  
process (e.g. random forest) + engine (the package to use) + fit it!
  
```{r}

abalone_xgboost <- parsnip::boost_tree(mode = "regression",mtry = 4, trees = 2000) %>% 
  parsnip::set_engine("xgboost") %>% 
  fit(age ~ ., data = baked_abalone)

abalone_rf <- parsnip::rand_forest(mode = "regression",mtry = 4, trees = 2000) %>% 
  parsnip::set_engine("ranger") %>% 
  fit(age ~ ., data = baked_abalone)

abalone_lm <- parsnip::linear_reg(mode = "regression") %>% 
  parsnip::set_engine("stan") %>% 
  fit(age ~ ., data = baked_abalone)


```
  
---

# parsnip

To make things even better, `parsnip` has standardized the predict function! Every prediction from parsnip comes back as a tibble with the same number of rows as the data passed to predict (by default with column )


```{r}

analysis_preds <- baked_abalone %>% 
  bind_cols(
    predict(abalone_xgboost, new_data = baked_abalone)
  )

analysis_preds %>% 
  ggplot(aes(age, .pred)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  geom_abline(slope = 1, intercept = 0)

```


---


# parsnip

```{r}

assessment_preds <- baked_assessment_abalone %>% 
  bind_cols(
    predict(abalone_lm, new_data = baked_assessment_abalone)
  )

assessment_preds %>% 
  ggplot(aes(age, .pred)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  geom_abline(slope = 1, intercept = 0)

```

---

# parsnip

```{r}

assessment_preds %>% 
  mutate(split = "assessment") %>% 
  bind_rows(analysis_preds %>% mutate(split = "analysis")) %>% 
  ggplot(aes(age, .pred, color = split)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  geom_abline(slope = 1, intercept = 0)


```


---


# yardstick


```{r}

rf_pred <- predict(abalone_rf, new_data = baked_assessment_abalone) %>% 
  rename(rf_pred = .pred)

lm_pred <- predict(abalone_lm, new_data = baked_assessment_abalone) %>% 
    rename(lm_pred = .pred)

evaluate <- baked_assessment_abalone %>% 
  bind_cols(rf_pred, lm_pred) %>% 
  pivot_longer(cols = contains("_pred"), names_to = "model", values_to = ".pred")

evaluate %>% 
  group_by(model) %>% 
  yardstick::rsq(age, .pred)



```



---

# Hyperparameter Profiling


---


# 
