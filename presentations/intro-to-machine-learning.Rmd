---
title: "Intro to Machine Learning with R"
subtitle: "Very Intro."
author: "Dan Ovando"
institute: "University of Washington"
date: "`r Sys.Date()`"
output:
  xaringan::moon_reader:
    css: ["default","default-fonts","blue-template/css/blue-template.css"]
    lib_dir: libs
    nature:
      beforeInit: "blue-template/macros.js"
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      slideNumberFormat: |
        <div class="progress-bar-container">
          <div class="progress-bar" style="width: calc(%current% / %total% * 100%);">
          </div>
        </div>`
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(fig.align = "center", dev = "svg", fig.height = 5, fig.width = 6)
library(tidyverse)
library(tidymodels)
library(here)
library(knitr)
library(furrr)
library(hrbrthemes)

pres_theme <- theme_ipsum(axis_title_size = 22,
                          base_size = 18,
                          plot_title_size = 24)

theme_set(pres_theme)

```


# Intro to Machine Learning: Objectives

We only have three hours! You won't leave here an expert in ML

My goals for you: 

1. Understand the motivation for machine learning

2. Understand the basic workflow of predictive modeling

3. Gain familiarity with the core predictive modeling tools in R

4. Develop some healthy skepticism around ML

---

# What is Machine Learning?

```{r, echo = FALSE, fig.align="center"}
knitr::include_graphics("https://imgs.xkcd.com/comics/machine_learning.png")
```


---

# What is Machine Learning?


.pull-left[

Artificial Intelligence has been around for a long time!

>The Analytical Engine has no pretensions whatever to originate anything. It can do whatever we know how to order it to perform.... Its province is to assist us in making available what we’re already acquainted with
>
> `r tufte::quote_footer('--- Lady Ada Lovelace, 1843')`

]
.pull-right[
`r include_graphics(here::here("presentations","imgs","ai-ml-dl.png"))`
]

.footnote[
Chollet & Allaire 2018 - Deep Learning with R
]

???
In reference to a mechanical computer called the Analytical Engine, referenced by Alan Turing
---

# What is Machine Learning?


.pull-left[

Machine learning advanced structural AI by asking whether computers could **learn** new tasks. 

- "Classical" programming says "if email contains 'get rich quick!' mark as spam

- Machine learning asks given known examples of spam, can computer find rules to identify it?

]
.pull-right[
`r include_graphics(here::here("presentations","imgs","ai-ml-dl.png"))`
]

.footnote[
Chollet & Allaire 2018 - Deep Learning with R
]

???
In reference to a mechanical computer called the Analytical Engine, referenced by Alan Turing
---

# Why Machine Learning?

.pull-left[
- You only care about prediction

- You suspect there's "hidden" information in your data
  - **lots** of plausible variables
  - Complex interactions and non-linearities

- Performance really matters
  - 0.5% increase in accuracy may be **REALLY** valuable to a company

- Image/text processing

- You want to make some sweet art

] 
--
.pull-right[

```{r, echo = FALSE}
include_graphics(here("presentations","imgs","aiart.jpg"))
```

.footnote[[artofericwayne](https://artofericwayne.com/2015/07/08/google-deep-dream-getting-too-good/)]
]


---

# ML for Predictive Modeling

The primary application for machine learning is **prediction**, not "understanding"

Modeling for Prediction: "Will it rain?"
  - Are people carrying umbrellas? Probably gonna rain. 
  - Test by out-of-sample prediction
  - Realm of machine learning
  
Modeling for Inference: "Why does it rain?"
  - Probably not because of umbrellas. 
  - Test by experimentation
  - Realm of statistics
  
Correlation does not equal causation, but some correlations can be great predictors  

**Big red flag: Works claiming to use machine learning for "why" questions**

???

IMO, ecology sometimes has a bad habit of worst of both worlds: low R<sup>2</sup> and no identification strategy

---

# The Predictive Modeling Workflow

1. Obtain labeled dataset

2. Split into training and testing sets
  - Lock the testing set
  
3. Use the training set to train and select final model
  - Split into analysis and assessment sets

4. Apply final model to testing set and see how you did


---

# Predictive Modeling

"Predictive" nature of most ML makes it much more engineering than theory focused.

The proof is in the pudding: If it predicts well, who cares why?

The focus goes from identification strategies and probability theory to

- How good is the training dataset?

- How valid is the training/testing 

---

# Predictive Modeling
I sometimes use ML as a test for "ceiling" of predictability given data

  - Helps learn how much predictive power insight is costing you

```{r, out.height=400, echo = FALSE}
 include_graphics(here::here("presentations","imgs","skynet.png"))
```

---

# Predictive Modeling

Predictive modeling falls under "supervised learning" (as opposed to unsupervised, things like clustering algorithms)

Regression (continuous response)
  - Predict presence of marine mammals
  - Predict sales on a given day
  - Predict salmon returns
  - Judged by things like mean squared error

Classification (categorical response)
  - What kind of fish is this?
  - Does a person have a disease?
  - Judged by things like classification accuracy

???
This branch of machine learning consists of finding interesting transformations of the input data without the help of any targets, for the purposes of data visualization, data compression, or data denoising, or to better understand the correlations present in the data at hand

Think PCA

---

# A Bestiary of Machine Learning Models

.pull-left[
Nearly all ML methods can be used for regression or classification

- Random forests
  - [`ranger`](https://github.com/imbs-hl/ranger)

- Gradient boosting machines
  - [`xgboost`](https://xgboost.readthedocs.io/en/latest/)

- Neural Networks
  - [`keras/TensorFlow`](https://tensorflow.rstudio.com/)
] .pull-right[

There are lots of them....

```{r}
caret::getModelInfo() %>% names()
```


]

---

# Random Forests

> "Bunch of stupid trees learning to correct errors of each other"
>
> - Vasily Zubarev

.pull-left[
.small[
Random forests (Breiman 2001) are a reliable workhorse of the ML world
- Not always the best, but usually close
- Rarely the worst
- Pretty hard to mess up too badly

Random forests are ensembles of regression trees

  - Algorithm learns split variables and breakpoints
  
  - Each tree is low-bias, high-variance
  
  - A "forest" of trees is low-bias, low(er)-variance
]

] .pull-right[

`r include_graphics(here::here("presentations","imgs","nico.png"))`

.footnote[Gutiérrez, Hilborn, & Defeo 2011]

]

---


# Random Forests

```{r, out.height=400, echo = FALSE}
 include_graphics(here::here("presentations","imgs","rf.png"))
```

.footnote[Kuhn & Johnson 2016]

---

# The Predictive Modeling Framework

1. Split into testing and training data. Split training data into analysis and assessment splits
  - [`rsample`](https://tidymodels.github.io/rsample/)

2. Preprocess data in analysis splits
  - [`recipes`](https://tidymodels.github.io/recipes/)
  - For extra credit, blind the data
  
3. Fit models to analysis splits, test on assessment
  - [`purrr`](https://purrr.tidyverse.org/)
  - [`parsnip`](https://www.tidyverse.org/blog/2018/11/parsnip-0-0-1/)
  - [`yardstick`](https://tidymodels.github.io/yardstick/)
  
4. Fit selected model on preprocessed training data, test on testing data
  - [`parsnip`](https://www.tidyverse.org/blog/2018/11/parsnip-0-0-1/)
  - Cross fingers


---

# The Predictive Modeling Framework


```{r, echo = FALSE}
include_graphics(here::here("presentations","imgs","flow.png"))
```

.footnote[From Max Kuhn's Predictive Modeling Workshop]

---

# Why R for ML?

I thought I was supposed to use Python for machine learning?

Very few (no?) ML models are actually **written** in R. Most in versions of C++/Python/CUDA. 

But, nearly all have interfaces with other languages. That means you can do all the data-wrangling in R, pass to ML models, then process and plot results in R

  - R is really good at this
  
Python is more common for these things so a bit more documentation, but documentation is also denser. 
  - Might be worth exploring if you're dealing with big data


---

# Our example data

We're going to use some data on abalone ages from the [UCI Machine Learning Repository](http://archive.ics.uci.edu/ml/datasets/Abalone), accessed through the `AppliedPredictiveModeling` package. 

```{r}
library(AppliedPredictiveModeling)
data(abalone)
abalone <- abalone %>% 
  janitor::clean_names()
glimpse(abalone)
```


---


# Abalone Ages

Age in years is rings + 1.5

```{r, echo = FALSE}

abalone <- abalone %>% 
  mutate(age = rings + 1.5) %>% 
  select(-rings)

abalone %>% 
  ggplot(aes(longest_shell, age, fill = type)) + 
  geom_point(shape  = 21, alpha = 0.75, size = 2) -> abplot

abplot
```


---


# Step 1a: Split into testing and training

We've got a solid amount of data here, so let's hold out 25% of our data for "testing"

Only real advantage of rsample::initial_split is memory (and some nice time features)

```{r}
abalone_split <- rsample::initial_split(abalone, prop = 0.75)
abalone_split
```

Could also do stratified sampling if desired

```{r}
abalone_strata_split <-
  rsample::initial_split(abalone,
                         prop = 0.9,
                         strata = "age",
                         breaks = 6)

```

---


# Step 1b: Analysis and Assessment splits


```{r, echo = FALSE, out.height=400, out.width=300}

include_graphics(here("presentations","imgs","overfit.png"))

```

.footnote[Kuhn & Johnson 2016]

---

# Step 1b: Analysis and Assessment splits

We now need to break our training data into analysis and assessment splits. 

We'll use these splits to tune and compare candidate models. 
  - How deep should the tree go?
  - How many layers should our Neural Net have?

There are lots of options for this most common being k(v)-fold cross validation 

```{r, echo = FALSE}

include_graphics(here("presentations","imgs","vfold.png"))


```

.footnote[Kuhn & Johnson 2016]

---


# Step 1b: Analysis and Assessment splits


```{r}

abalone_vfold <- abalone_split %>% 
  rsample::training() %>% 
  rsample::vfold_cv(v = 5, repeats = 1)

abalone_vfold 

```

---


# Step 2: Analysis and Assessment splits

Can also stratify if we're worried about rare ages


```{r}

abalone_vfold <- abalone_split %>% 
  rsample::training() %>% 
  rsample::vfold_cv(v = 5, repeats = 2, strata = age, breaks = 6)

abalone_vfold %>% 
  mutate(dat = map(splits, analysis))

```

---

# Step 2: Preprocessing (feature engineering)

Statistics usually refers to variable selection,transformations/encodings etc. as "preprocessing"

Just because, computer science usually calls if "feature engineering"

Examples

  - Centering and scaling
  
  - PCA
  
  - Dummy variables
  
  - Interactions
  
  - Imputation
  
How can we do this on the abalone data?

---


# Step 2: Preprocessing

**Absolutely critical to avoid data leakages**

Common mistake: Center and scale the data, then split, train etc. 

  - The "testing" data has leaked into the mean used in the "training" data
  
We always want to do our preprocessing on the most "inner" step of our resampling, e.g. each v-fold split. 

enter `recipes`!


```{r, echo = FALSE}
include_graphics(here("presentations","imgs","recipes.png"))

```


---


# `recipes`

`recipes` is an incredibly handy package for breaking preprocessing steps into a series of pipe-able operations

  - Allows for transformations from training data to be applied to testing data
  
  - For now, let's pull out one "analysis" split
  
```{r}

analysis_data <- rsample::analysis(abalone_vfold$splits[[1]])

assessment_data <- rsample::assessment(abalone_vfold$splits[[1]])

age_recipe <- recipe(age ~ ., data = analysis_data)

age_recipe

```

---

# `recipes`

Preprocessing operations are specified by `step_` functions

Suppose we want to center and scale all the numeric predictors, and add dummy variables for "type


```{r}

age_recipe <- recipe(age ~ ., data = analysis_data) %>%
  step_center(all_numeric(), -all_outcomes()) %>% 
  step_scale(all_numeric(),-all_outcomes()) %>% 
  step_dummy(all_nominal()) 
 

age_recipe

```

---

# `recipes`

We now have a portable recipe that we can apply to new datasets. 

To follow the theme, we can now "prepare" (`prep`) the recipe using our analysis data. 

Prepping calculates and stores the steps of the recipe using the supplied data 


```{r}

prepped_abalone <-
  prep(age_recipe, data = analysis_data)

prepped_abalone
```


---

# `recipes`

Once we've prepped our recipe, we can now apply it to data using `bake`


```{r}

baked_abalone <- bake(prepped_abalone, new_data = analysis_data)

glimpse(baked_abalone)

```

---


# `recipes`

Importantly, `bake` allows you to apply the steps "prepped" from one dataset to another!

In this case, we can now bake our assessment data using, using the means and standard deviations from the analysis split contained in our prepped recipe.  

```{r}

baked_assessment_abalone <- bake(prepped_abalone, new_data = assessment_data)

glimpse(baked_assessment_abalone)

```

---


# Exercise

Explore alternative `step_` functions in recipes. What does step_knn do?

---


# Step 3 Tune and Select Models

OK we're getting pretty close. We now have a preprocessed analysis and assessment splits. 

What should we use? We saw that there are **tons** of ML models out there, and different versions of the same model. 

  - Not always clear reasons to pick one over the other
  
We want to try lots of different models, but this can be a pain....

---

# Step 3 Tune and Select Models

```{r, echo = TRUE, eval = FALSE}
# From randomForest
rf_1 <-
  randomForest(x,y,mtry = 12,ntree = 2000,importance = TRUE)

# From ranger
rf_2 <- ranger(
  y ~ ., 
  data = dat, 
  mtry = 12, 
  num.trees = 2000, 
  importance = 'impurity'
)

# From sparklyr
rf_3 <- ml_random_forest(
  dat, 
  intercept = FALSE, 
  response = "y", 
  features = names(dat)[names(dat) != "y"], 
  col.sample.rate = 12,
  num.trees = 2000
)
```
---


# enter `parsnip`

The old solution to this problem was `caret`.
  - Centralized package for tuning and fitting models
  - Bit of a black box
  - Inconsistent naming conventions
  
`parsnip` to the rescue!

```{r, echo = FALSE, out.height=300, out.width=300}
include_graphics(here("presentations","imgs","parsnip.png"))
```

---


# `parsnip`

`parsnip` breaks the model fitting process into a series of pipe-able steps. 
  - Also standardized naming inputs!
  
process (e.g. random forest) + engine (the package to use) + fit it!
  
```{r}
abalone_rf <-
  parsnip::rand_forest(
    mode = "regression",
    mtry = ncol(baked_abalone) - 2,
    trees = 2000,
    min_n = 2
  ) %>%
  parsnip::set_engine("ranger",
                      splitrule = "extratrees") %>%
  fit(age ~ ., data = baked_abalone)

```
  
---

# parsnip - analysis fit

To make things even better, `parsnip` has standardized the predict function!
  - Always has same number of rows as data

.pull-left[

```{r}

analysis_preds <- baked_abalone %>%
  bind_cols(predict(abalone_rf,
    new_data = baked_abalone))
```


] .pull-right[

```{r, echo = FALSE}

analysis_preds %>%
  ggplot(aes(age, .pred)) +
  geom_point() +
  geom_smooth(method = "lm") +
  geom_abline(slope = 1, intercept = 0)
```

]


---


# parsnip - assessment fit

.pull-left[
```{r}

assessment_preds <- 
  baked_assessment_abalone %>% 
  bind_cols(
    predict(abalone_rf, 
    new_data = baked_assessment_abalone)
  )

```
] .pull-right[
```{r}
assessment_preds %>% 
  ggplot(aes(age, .pred)) + 
  geom_point() + 
  geom_smooth(method = "lm") + 
  geom_abline(slope = 1, intercept = 0)

```
]

---

# parsnip - Analysis vs. Assessment

```{r, echo = FALSE}

assessment_preds %>% 
  mutate(split = "assessment") %>% 
  bind_rows(analysis_preds %>% mutate(split = "analysis")) %>% 
  ggplot(aes(age, .pred, color = split)) + 
  geom_jitter(alpha = 0.75) + 
  geom_smooth(method = "lm") + 
  geom_abline(slope = 1, intercept = 0)


```


---


# yardstick

The yardstick package has a bunch of useful ways of measuring performance


```{r}

rf_pred <- predict(abalone_rf, new_data = baked_assessment_abalone) %>% 
  rename(rf_pred = .pred)

evaluate <- baked_assessment_abalone %>% 
  bind_cols(rf_pred) %>% 
  pivot_longer(cols = contains("_pred"), names_to = "model", values_to = ".pred")

evaluate %>% 
  group_by(model) %>% 
  summarise(rsquared = yardstick::rsq_vec(truth = age, estimate = .pred),
            rmse = yardstick::rmse_vec(truth = age, estimate = .pred))


```

---

# Hyperparameter Profiling

We get an out-of-bag R<sup>2</sup> of around 0.55

Need to find "best" hyperparameters for random forest

  - mtry: number of possible variables to randomly select at each split
  - min_n: the smallest allowable terminal node size
  - splitrule: the rule used for choosing variables to split by splitting 
  - trees: the number of trees in the forest
  
Back to purrr!


---


# Hyperparameter Profiling - gotta try them all!

```{r}

abalone_vfold$sampid <-
  paste0(abalone_vfold$id, abalone_vfold$id2, sep = '-')

param_grid <- tidyr::expand_grid(
  mtry = seq(1,(ncol(baked_abalone) - 2), by = 2),
  trees = seq(100, 2000, by = 1000),
  splitrule = c("variance", "extratrees"),
  min_n = c(2,10),
  sampid = unique(abalone_vfold$sampid)
) %>%
  left_join(abalone_vfold, by = "sampid")


```

---

# Hyperparameter Profiling - Function Style


```{r, echo = FALSE}
param_grid
```


---

# Hyperparameter Profiling - Function Style

* See .Rmd
```{r, cache=TRUE}


fit_foo <- function(mtry, trees, splitrule, min_n, split, recipe, use = "summary"){
  
  analysis_data <- rsample::analysis(split)
  
  assessment_data <- rsample::assessment(split)
  
  prepped_abalone <- prep(recipe, data = analysis_data)
  
  baked_abalone <- bake(prepped_abalone, new_data = analysis_data)
  
  baked_assessment_abalone <-
    bake(prepped_abalone, new_data = assessment_data)
  
  abalone_rf <-
    parsnip::rand_forest(
      mode = "regression",
      mtry = mtry,
      trees = trees,
      min_n = min_n
    ) %>%
    parsnip::set_engine("ranger",
                        splitrule = splitrule) %>%
    fit(age ~ ., data = baked_abalone)
  
  rf_pred <-
    predict(abalone_rf, new_data = baked_assessment_abalone) %>%
    rename(rf_pred_age = .pred) %>%
    bind_cols(assessment_data)
  
  rf_summary <- rf_pred %>%
    summarise(
      rsquared = yardstick::rsq_vec(age, rf_pred_age),
      rmse = yardstick::rmse_vec(age, rf_pred_age)
    )
  
  all_preds <-
    predict(abalone_rf, new_data = baked_assessment_abalone) %>%
    rename(rf_pred_age = .pred) %>%
    bind_cols(assessment_data) %>%
    mutate(split = "assessment") %>%
    bind_rows(
      analysis_data %>%
        bind_cols(predict(abalone_rf, new_data = baked_abalone)) %>%
        rename(rf_pred_age = .pred) %>%
        mutate(split = "analysis")
    )
  
  if (use == "summary") {
    rf_summary
  } else {
    out <- list(
      fit = abalone_rf,
      pred = rf_pred,
      summary = rf_summary,
      all_preds = all_preds
    )
  }
  
}

future::plan(future::multiprocess, workers = 6)

param_grid <- param_grid %>% 
  mutate(rf_fit = future_pmap(list(mtry = mtry, 
                            trees = trees,
                            splitrule = splitrule,
                            min_n = min_n,
                            split = splits),
                       fit_foo, 
                       recipe = age_recipe,
                       .progress = TRUE))

```


---


# Hyperparameter Profiling - Function Style

```{r, echo = FALSE}

param_grid %>% 
  unnest(cols = rf_fit) %>% 
  group_by(mtry, trees, min_n, splitrule) %>% 
  summarise(rmse = mean(rmse)) %>% 
  ggplot(aes(mtry, rmse, color = factor(trees))) + 
  geom_point() +
  facet_grid(min_n ~ splitrule)

```

---

# Step 4. Fit training data with tuned hyperparameters

Different problems might require different metrics
```{r}

best_params <- param_grid %>% 
  unnest(cols = rf_fit) %>% 
  group_by(mtry, trees, min_n, splitrule) %>% 
  summarise(rmse = mean(rmse)) %>% 
  ungroup() %>% 
  filter(rmse == min(rmse))

best_fit <- fit_foo(mtry = best_params$mtry, trees = best_params$trees,
                    splitrule = best_params$splitrule,
                    min_n = best_params$min_n,
                    recipe = age_recipe,
                    split = abalone_split, 
                    use = "results")

best_fit$summary$rmse

```

---

# Comparison to Baseline

"Artificial Intelligence Reveals the Age of Abalone" - Nature here we come!

Always a good idea to have a baseline to compare predictive model performance to

Simple idea:  linear model using the same variables as passed to ranger. 

```{r}

training_abalone <- training(abalone_split)

testing_abalone <- testing(abalone_split)

prepped_abalone <- prep(age_recipe, data = training_abalone)
  
baked_abalone <- bake(prepped_abalone, new_data = training_abalone)
  
baked_test_abalone <- bake(prepped_abalone, new_data = testing_abalone)

simple_abalone <- lm(age ~ ., data = baked_abalone)

```


---

# Comparison to Baseline

```{r}

training_abalone$pred <- predict(simple_abalone, newdata = baked_abalone)

testing_abalone$pred <- predict(simple_abalone, newdata = baked_test_abalone)

yardstick::rmse(data = testing_abalone, truth = age, estimate = pred)

```

---

# Comparison to Baseline

```{r, echo = FALSE}

lm_fit <- training_abalone %>%
  mutate(split = "analysis") %>%
  bind_rows(testing_abalone %>%
              mutate(split = "assessment")) %>% 
  mutate(model = "lm")

best_fit$all_preds %>% 
  mutate(model = "Random Forest") %>% 
  rename(pred = rf_pred_age ) %>% 
  bind_rows(lm_fit) %>% 
  ggplot(aes(age, pred, color = model)) + 
  geom_abline(slope = 1, intercept = 0, linetype = 2) +
  geom_point() + 
  geom_smooth(method = "lm") +
  facet_wrap(~split)

```


---

# Comparison to Baseline

```{r, echo = FALSE}
knitr::include_graphics("https://media.giphy.com/media/q36oyUGLdB1Mk/giphy.gif") 
```


---

# All the Machines Are Belong to Us

The hard part of (implementing) machine learning isn't the machine learning!

  - Often as simple as changing a line of code in parsnip call

Good machine learning requires
  - The right problems
  - Careful design and quarantine of testing and training splits
  - Good feature engineering
  - Reasonable benchmarks
  - Domain knowledge!
  
**ML will help you squeeze more information from your data**
**It can't create information that isn't there**

---

# Exercise

See if you can improve on ML predictions of Abalone age

Try different feature engineering
  - remove correlated variables?
  - upsample/downsample?
  - log transform?

Try different models/engines
  - xgboost
  - mars

---

# An Example to Show Off ML

[see keras intro](https://keras.rstudio.com/)
---

